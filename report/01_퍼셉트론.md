# 1. 퍼셉트론(perceptron)

- 퍼셉트론(Perceptron)은 프랑크 로젠블란트(Frank Rosenblatt)가 1957년에 제안한 **초기 형태의 인공 신경망으로 다수의 입력으로부터 하나의 결과를 내보내는 알고리즘**이다.

![https://wikidocs.net/images/page/24958/%EB%89%B4%EB%9F%B0.PNG](https://wikidocs.net/images/page/24958/%EB%89%B4%EB%9F%B0.PNG)

- 뉴런을 예로 들수가 있는데, 뉴런은 **가지돌기에서 신호를 받아들이**고, 이 신호가 일정치 이상의 크기를 가지면 **축삭돌기를 통해서 신호를 전달**한다.

![https://wikidocs.net/images/page/24958/perceptrin1_final.PNG](https://wikidocs.net/images/page/24958/perceptrin1_final.PNG)

- 다수의 입력을 받는 퍼셉트론의 그림이다.
- 신경 세포 뉴런의 입력 신호와 출력 신호가 퍼셉트론에서 각각 입력값과 출력값에 해당된다.
- x는 입력값을 의미하며, W는 가중치, y는 출력값이다. 해당 원은 인공 뉴런에 해당된다.
- 실제 신경 세포 뉴런에서의 신호를 전달하는 **축삭돌기의 역할**을 퍼셉트론에서는 `가중치`가 대신한다. 각각의 인공 뉴런에서 보내진 입력값 x는 각각의 가중치 W와 함께 종착지인 인공 뉴런에 전달되고 있다.
- **각각의 입력값에는 각각의 가중치가 존재하는데, 이 때 가중치의 값이 크면 클수록 해당 입력 값이 중요하다는 것을 의미한다.**
- 각 입력값이 가중치와 곱해져서 인공 뉴런에 보내지고, 각 입력값과 그에 해당되는 가중치의 곱의 **전체 합이 임계치(threshold)를 넘으면** 종착지에 있는 **인공 뉴런은 출력 신호로서 1을 출력**하고, **그렇지 않을 경우에는 0을 출력**한다. 이러한 함수를 **계단 함수(Step Function)**라고 한다.
- 계단 함수에 사용된 임계치값을 수식으로 표현할 때는 보통 세타(Θ)로 표현한다.

![https://wikidocs.net/images/page/24958/perceptron2_final.PNG](https://wikidocs.net/images/page/24958/perceptron2_final.PNG)

- 단, 위의 식에서 임계치를 좌변으로 넘기고 편향 b(bias)로 표현할 수도 있다. 편향 b또한 퍼셉트론의 입력으로 사용된다. 보통 그림으로 표현할 때는 입력값이 1로 고정되고, 편향 b가 곱해지는 변수로 표현된다.
- 편향 b 또한 딥 러닝이 최적의 값을 찾아야 할 변수 중 하나이다.
- 이렇게 뉴런에서 출력값을 변경시키는 함수를 활성화 함수(Activation Function)라고 한다. 초기 인공 신경망 모델인 퍼셉트론은 활성화 함수로 계단 함수를 사용했지만, 그 뒤에 등장한 여러가지 발전된 신경망들은 계단 함수 외에도 여러 다양한 활성화 함수를 사용하기 시작했다. ex ) 시그모이드, 소프트맥스
- 로지스틱 회귀 모델이 인공 신경망에서는 하나의 인공 뉴런으로 볼 수 있다. 로지스틱 회귀를 수행하는 인공 뉴런과 퍼셉트론의 차이는 오직 활성화 함수의 차이이다.
  - 인공 뉴런 : 활성화 함수
  - 퍼셉트론 : 계단 함수

# 1. 단층 퍼셉트론(Single-Layer Perceptron)

![https://wikidocs.net/images/page/24958/perceptron3_final.PNG](https://wikidocs.net/images/page/24958/perceptron3_final.PNG)

- 퍼셉트론은 단층 퍼셉트론과 다층 퍼셉트론으로 나누어지는데, 단층 퍼셉트론은 값을 보내는 단계와 값을 받아서 출력하는 두 단계로만 이루어진다. 이때 이 각 단계를 보통 층(layer)이라고 부르며, 이 두개의 층을 입력층(input layer)과 출력층(output layer)이라고 한다.
- 단층 퍼셉트론을 이용하면 AND, NAND, OR 게이트를 쉽게 구현할 수 있다.
- AND 게이트의 경우에는 두 개의 입력 값이 모두 1인 경우에만 출력값이 1이 나오는 구조를 가지고 있다.
- AND 게이트는 출력 x1,x2(다수의 입력값)이 존재하며, 임계치를 넘으면 1을 출력, 넘기지 못하면 0을 출력하는 게이트를 사용하면 되는데, 이 때 입력값들의 w1,w2(가중치: 입력값의 개수와 같아야 한다.)와 편향(b)는 어떻게 표현될 수 있을까?

```python
def AND_gate(x1, x2):
    w1=0.5
    w2=0.5
    b=-0.7
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1
```

- 가중치는 여러 형태를 가질 수 있지만, [0.5, 0.5, -0.7]과 같은 형태를 가질 수도 있다. 그러면 두 개의 입력값이 1인 경우에만 출력값이 0, 나머지 입력값의 쌍에 대해서는 모두 출력값이 1이 나오는 NAND 게이트는 어떨까?

```python
def NAND_gate(x1, x2):
    w1=-0.5
    w2=-0.5
    b=0.7
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1
```

- 단지 같은 코드에 함수 이름과 가중치와 편향만 변화했다. 퍼셉트론의 구조 자체는 같기 때문이다.
- OR 게이트 또한 적절한 가중치와 편향 값만 찾으면 단층 퍼셉트론의 식으로 구현할 수 있다.

```python
def OR_gate(x1, x2):
    w1=0.6
    w2=0.6
    b=-0.5
    result = x1*w1 + x2*w2 + b
    if result <= 0:
        return 0
    else:
        return 1
```

![https://wikidocs.net/images/page/24958/oragateandnandgate.PNG](https://wikidocs.net/images/page/24958/oragateandnandgate.PNG)

- 이렇듯 AND, NAND, OR 게이트는 단층 퍼셉트론으로 구성이 가능하다. 하지만 단층 퍼셉트론으로 구현이 불가능한 게이트가 있는데, 바로 XOR 게이트이다. 위의 퍼셉트론 구조에서 수많은 가중치와 편향을 넣어봐도 XOR 게이트를 구현하는 것은 불가능하다. 그 이유는 **단층 퍼셉트론은 직선 하나로 두영역을 나눌 수 있는 문제에 대해서만 구현이 가능하기 때문**이다.

![https://wikidocs.net/images/page/24958/xorgraphandxorgate.PNG](https://wikidocs.net/images/page/24958/xorgraphandxorgate.PNG)

![https://wikidocs.net/images/page/24958/xorgate_nonlinearity.PNG](https://wikidocs.net/images/page/24958/xorgate_nonlinearity.PNG)

- 하얀색 원과 검은색 원을 직선 하나로 나누는 것은 불가능하다. 즉, 단층 퍼셉트론으로는 XOR 게이트를 구현하는 것이 불가능하다.
- 이를 단층 퍼셉트론은 선형 영역에 대해서만 분리가 가능하다고 말한다.
- 다시 말하면 XOR 게이트는 직선이 아닌 곡선. 비선형 영역으로 분리하면 구현이 가능하다.

# 2. 다층 퍼셉트론(Multi-Layer Perceptron)

![https://wikidocs.net/images/page/24958/perceptron_4image.jpg](https://wikidocs.net/images/page/24958/perceptron_4image.jpg)

```python
def XOR_gate(x1, x2):
    nx = NAND_gate(x1, x2)
    ox = OR_gate(x1, x2)
    result = AND_gate(nx,ox)
    return result;
```

- XOR 게이트는 기존의 AND, NAND, OR 게이트를 조합하면 만들 수 있다. 퍼셉트론 관점에서 말하면, 층을 더 쌓으면 만들 수 있다.
- 다층 퍼셉트론은 중간에 층을 더 추가한다는 점에서 단층 퍼셉트론과의 차이점이 있다.
- 이렇게 입력층과 출력층 사이에 존재하는 층을 은닉층(hidden layer)이라고 한다.
- XOR 예제에서는 은닉층 1개만으로 문제를 해결할 수 있었지만, 다층 퍼셉트론은 본래 은닉층이 1개 이상인 퍼셉트론을 말한다.
- 즉, XOR 문제보다 더욱 복잡한 문제를 해결하기 위해서 다층 퍼셉트론은 중간에 수많은 은닉층을 더 추가할 수 있다.

![https://wikidocs.net/images/page/24958/%EC%9E%85%EC%9D%80%EC%B8%B5.PNG](https://wikidocs.net/images/page/24958/%EC%9E%85%EC%9D%80%EC%B8%B5.PNG)

- 이와 같이 은닉층이 2개 이상인 신경망을 심층 신경망(Deep Neural Network, DNN)이라고 한다.
- 심층 신경망은 다층 퍼셉트론만 이야기 하는 것이 아니라, 여러 변형된 다양한 신경망들도 은닉층이 2개 이상이 되면 심층 신경망이라고 한다.
- 지금까지는 OR, AND, XOR 게이트 등. 퍼셉트론이 가야할 정답을 참고로 퍼셉트론이 정답을 출력할 때까지 가중치를 바꿔보면서 맞는 가중치를 찾았다.(수동) 하지만 이제는 기계가 가중치를 스스로 찾아내도록 자동화시켜야 한다. 이것이 머신 러닝에서 말하는 학습(training) 단계에 해당된다.
- 학습에는 손실함수(Loss function)와 옵티마이저(Optimizer)를 사용한다. 그리고 만약 학습을 시키는 인공 신경망이 심층 신경망일 경우 이를 심층 신경망을 학습시킨다고 하여, 딥 러닝(Deep Learning)이라고 한다.
