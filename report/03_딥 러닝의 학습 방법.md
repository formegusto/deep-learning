# How deep learning learns

# 1. 순전파 (Foward Propagation)

![https://wikidocs.net/images/page/36033/%EC%88%9C%EC%A0%84%ED%8C%8C.PNG](https://wikidocs.net/images/page/36033/%EC%88%9C%EC%A0%84%ED%8C%8C.PNG)

- 활성화 함수, 은닉층의 수, 각 은닉층의 뉴런 수 등 딥 러닝 모델을 설계하고 나면 입력값은 입력층, 은닉층을 지나면서 각 층에서의 가중치와 함께 연산되며, 출력층으로 향한다. 그리고 출력층에서 모든 연산을 마친 예측값이 나오게 된다.
- 이와 같이 입력층에서 출력층 방향으로 예측값의 연산이 진행되는 과정을 순전파라고 한다.

# 2. 손실 함수 (Loss function)

![https://wikidocs.net/images/page/36033/%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98.PNG](https://wikidocs.net/images/page/36033/%EC%86%90%EC%8B%A4%ED%95%A8%EC%88%98.PNG)

- **손실 함수는 실제값과 예측값의 차이를 수치화해주는 함수**이다. 이 두값의 차이. 즉, **오차가 클수록 손실 함수의 값은 크고, 오차가 작을 수록 손실 함수의 값은 작아진다.**
- 회귀에서는 평균 제곱 오차, 분류 문제에서는 크로스 엔트로피를 주로 손실 함수로 사용한다. **손실 함수의 값을 최소화 하는 2개의 매개변수인 가중치 W와 편향 b를 찾아가는 것이 딥 러닝의 학습 과정이므로, 손실 함수의 선정은 매우 중요**하다.

## 1. MSE (Mean Squared Error, MSE)

- 오차 제곱 평균을 의미한다. 연속형 변수를 예측할 때 사용된다.

## 2. 크로스 엔트로피 (Cross-Entropy)

- 낮은 확률로 예측해서 맞추거나, 높은 확률로 예측해서 틀리는 경우 loss가 더 크다.
- keras의 model.complie() 에서는 이진 분류(Binary Classification)의 경우, binary_crossentropy를 사용한다.
- 다중 클래스 분류(Multi-Class Classification)일 경우, categorical_crossentropy를 사용한다.

# 3. 옵티마이저 (Optimizer)

![https://wikidocs.net/images/page/36033/%EC%97%AD%EC%A0%84%ED%8C%8C_%EA%B3%BC%EC%A0%95.PNG](https://wikidocs.net/images/page/36033/%EC%97%AD%EC%A0%84%ED%8C%8C_%EA%B3%BC%EC%A0%95.PNG)

- 손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라진다.
- 여기에 배치(Batch)라는 개념에 대한 이해가 필요하다. 배치는 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양을 말한다.
  - 전체 데이터를 가지고 혹은 정해준 양의 데이터를 가지고 매개 변수의 값을 조정할 수 있다.

### 1. 배치 경사 하강법 (Batch Gradient Descent)

- 가장 기본적인 경사 하강법, 옵티마이저 중 하나로, 오차(loss)를 구할 때 전체 데이터를 고려한다.
- 머신 러닝에서는 1번의 훈련 횟수를 1 에포크라고 하는데, 배치 경사 하강법은 한 번의 에포크에 모든 매개변수 업데이트를 단 한 번 수행한다.
- 이러한 특징은 전체 데이터를 고려해서 에포크 당 시간이 오래 걸리며, 메모리를 크게 요구한다는 단점이 있으나, 글로벌 미니멈을 찾을 수 있다.

### 2. 확률적 경사 하강법 (Stochastic Gradient Descent, SGD)

- 매개변수 값을 조정 시 전체 데이터가 아니라, 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법이다.
- 더 적은 데이터를 사용하므로, 더 빠르게 계산할 수 있다.
- 매개변수의 변경폭이 불안정하고, 때로는 배치 경사 하강법보다 정확도가 낮을 수도 있지만, 속도 만큼은 경사 하강법보다 빠르다는 장점이 있다.

### 3. 미니 배치 경사 하강법 (Mini-Batch Gradient Descent)

- 전체 데이터도 아니고, 1개의 데이터도 아니고 정해진 양에 대해서만 계산하여 매개변수의 값을 조정하는 경사 하강법을 미니 배치 경사하강법이라고 한다.
- 빠르고, SGD보다 안정적이다. 실제로 가장 많이 사용된다.

### 4. 모멘텀 (Momentum)

- 관성이라는 물리학의 법칙을 응용한 방법이다. 모멘텀 경사 하강법에 관성을 더 해준다. 모멘텀은 경사 하강법에서 계산된 접선의 기울기에 한 시점(step) 전의 접선의 기울기값을 일정한 비율만큼 반영한다.
- 이렇게 하면 마치 언덕에서 공이 내려올 때, 중간에 작은 웅덩이에 빠지더라도 관성의 힘으로 넘어서는 효과를 줄 수 있다.
- 다시 말해 로컬 미니멈에 도달하였을 때, 기울기가 0이라서 기존의 경사 하강법이라면 이를 글로벌 미니멈으로 잘 못 인식하여 계산이 끝났을 수도 있지만, 모멘텀 관성의 힘을 빌리면 값이 조절되면서 로컬 미니멈에서 탈출하는 효과를 얻을 수도 있다.

### 5. 아다그라드 (Adagrad)

- 매개변수들은 각자 의미하는 바가 다른데, 모든 매개변수에 동일한 학습률을 적용하는 것을 비효율적이다. 아다그라드는 각 매개변수에 서로 다른 학습률을 적용시킨다. 이 때 변화가 많은 매개변수는 학습률이 작게 설정되고, 변화가 적은 매개변수는 학습률을 높게 설정시킨다.

### 6. 알엠에스프롭 (RMSprop)

- 아다그라드를 개선한 방식

### 7. 아담 (Adam)

- 알엠에스프롭과 모멘텀 두 가지를 합친 듯한 방법으로, 방향과 학습률 두 가지를 모두 잡기 위한 방법이다.

# 4. 에포크와 배치 크기와 이터레이션 (Epochs and Batch size and Iteration)

- 기계는 실제값과 예측값의 오차로부터 옵티마이저를 통해서 가중치를 업데이트 한다. 머신 러닝에서는 이 과정을 학습이라고 한다.
- 하지만 사람과 같듯이, 기계도 같은 문제지와 정답지를 주더라도 공부 방법을 다르게 설정할 수 있다.

![https://wikidocs.net/images/page/36033/batchandepochiteration.PNG](https://wikidocs.net/images/page/36033/batchandepochiteration.PNG)

### 1. 에포크(Epoch)

- 에포크란 인공 신경망에서 전체 데이터에 대해서 순전파와 역전파가 끝난 상태를 말한다. 전체 데이터를 한아ㅢ 문제지에 비유한다면 문제지의 모든 문제를 끝까지 다 풀고, 정답지로 채점을 하여 문제지에 대한 공부를 한 번 끝낸 상태를 말한다.
- 에포크가 50이라고 하면, 전체 데이터 단위로는 총 50번 학습한 것이다. 이 에포크 횟수가 지나치거나 너무 적으면 과적합과 과소적합이 발생할 수 있다.

### 2. 배치 크기(Batch size)

- 몇 개의 데이터 단위로 매개변수를 업데이트 하는지를 말한다. 현실에 비유하면 문제지에서 몇 개씩 문제를 풀고나서 정답지를 확인하는냐의 문제이다.
- 기계 입장에서는 실제값과 예측값으로부터 오차를 계산하고, 옵티마이저가 매개변수를 업데이트 한다. 여기서 중요한 포인트는 업데이트가 시작되는 시점이 정답지/실제값을 확인하는 시점이라는 것이다.
- 이 때 배치 크기와 배치의 수는 다른 개념이다. 전체 데이터가 2000일 때, 배치 크기를 200으로 준다면 배치의 수는 10이다. 이는 에포크에서 배치 크기를 나눠준 값이기도 한다. 이 때 배치의 수를 이터레이션이라고 한다.

### 3. 이터레이션(Iteration)

- 이터레이션이란 한 번의 에포크를 끝내기 위해서 필요한 배치의 수를 말한다. 또는 한 번의 에포크 내에서 이루어지는 매개변수의 업데이트 횟수이기도 하다.
