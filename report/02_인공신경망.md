# 2. 인공 신경망 (Artificial Neural Network)

# 1. 피드 포워드 신경망(Feed-Forward NN: FFNN)

![https://wikidocs.net/images/page/24987/mlp_final.PNG](https://wikidocs.net/images/page/24987/mlp_final.PNG)

- 다층 퍼셉트론(MLP)과 같이 입력층에서 출력층 방향으로 연산이 전개되는 신경망을 피드 포워드 신경망이라고 한다. 이렇게 별도로 정의되는 이유는 FFNN이 아닌, 신경망이 존재하기 때문이다.

- 대표적으로 RNN이라는 신경망이 존재하는데, 이 신경망은 **은닉층의 출력값을 출력층으로도 값을 보내지만**, **동시에 은닉층의 출력값이 다시 은닉층의 입력으로 사용되는데** 이는 FFNN의 정의에서 벗어난다.

![https://wikidocs.net/images/page/24987/rnn_final.PNG](https://wikidocs.net/images/page/24987/rnn_final.PNG)

# 2. 전결합층(Fully-connected layer, FC, Dense layer)

- 다층 퍼셉트론은 은닉층과 출력층에 있는 모든 뉴런이 바로 이전 층의 모든 뉴런과 연결되어 있다. 이와 같이 어떤 층의 모든 뉴런이 이전 층의 모든 뉴련과 연결돼 있는 층을 전결합층이라고 한다.
- 즉, 앞서 본 다층 퍼셉트론의 모든 은닉층과 출력층은 전결합층이다. 이와 동일한 의미로 밀집층(Dense Layer) 라고 부르기도 하는데, 케라스에서 밀집층을 구현할 때 Dense() 를 사용한다.
- 만약 전결합층만으로 구성된 피드 포워드 신경망이 있다면, 이를 전결합 피드 포워드 신경망 이라고도 한다.

# 3. 활성화 함수(Activation Function)

- 앞서 배운 퍼셉트론에서는 계단함수를 통해 출력값이 0이 될지, 1이 될지를 결정했다.
- 이러한 매커니즘은 실제 뇌를 구성하는 신경 세포 뉴런이 전위가 일정치 이상이 되면 시냅스가 서로 화학적으로 연결되는 모습을 모방한 것이다. 이렇게 은닉층과 출력층의 뉴런에서 출력값을 결정하는 함수를 활성화 함수라고 한다.

## 1. 비선형 함수(Nonlinear function)

- 활성화 함수의 특징은 선형 함수가 아닌, 비선형 함수여야 한다는 점이다.
  - 선형 함수 : 출력이 입력의 상수배만큼 변하는 함수
  - 비선형 함수 : 직선 1개로는 그릴 수 없는 함수
- 인공 신경망의 능력을 높이기 위해서는 은닉층을 계속해서 추가해야 한다. 그런데 만약 활성화 함수로 선형 함수를 사용하게 되면 은닉층을 쌓을 수가 없다.
- 선형 함수를 활성화 함수로서 사용하게 되면 W*W*W*x 와 같은 형태가 된다. 그런데 이는 잘 생각해보면 W의 3제곱 * x 로 표현이 가능하다. 즉, 선형 함수로는 은닉층을 여러번 추가하더라도 1회 추가한 것과 차이를 줄 수 없다.
- 하지만 아예 의미가 없지는 않다. 학습 가능한 가중치가 새로 생긴다는 점에서 분명히 의미가 있다.
  - 그래서 선형 함수를 사용한 층을, 활성화 함수를 사용하는 은닉층과 구분하기 위해서 선형층이나 투사층 등의 다른 표현을 사용하여 표현하기도 한다.
  - 활성화 함수를 사용하는 일반적인 은닉층을 선형층과 대비되는 표현을 사용하면 비선형층이다.

## 2. 계단 함수(Step Function)

![Untitled](https://user-images.githubusercontent.com/52296323/125029164-360d8680-e0c4-11eb-9218-1a8529f04ae9.png)

```python
def step(x):
    return np.array(x > 0, dtype=np.int)
x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = step(x)
plt.title('Step Function')
plt.plot(x,y)
plt.show()
```

- 계단 함수는 이제 거의 사용되지 않는다.

## 3. 시그모이드 함수(sigmoid function)와 기울기 손실

![Untitled 1](https://user-images.githubusercontent.com/52296323/125029180-3ad23a80-e0c4-11eb-86f4-9dc32f8b89a6.png)

```python
# 시그모이드 함수 그래프를 그리는 코드
def sigmoid(x):
    return 1/(1+np.exp(-x))
x = np.arange(-5.0, 5.0, 0.1)
y = sigmoid(x)

plt.plot(x, y)
plt.plot([0,0],[1.0,0.0], ':') # 가운데 점선 추가
plt.title('Sigmoid Function')
plt.show()
```

- 시그모이드 함수의 문제점은 미분을 해서 기울기를 구할 때 발생한다.
- 시그모이드 함수는 출력값이 0또는 1에 가까워지면 그래프의 기울기가 완만해지는 모습을 볼 수 있다.
- 하지만 역전파 과정에서 0에 가까운 아주 작은 기울기가 곱해지게 되면, 앞단에는 기울기가 잘 전달되지 않게 된다. 이러한 현상을 기울기 손실(Vanishing Gradient) 문제 라고 한다.
- 시그모이드 함수를 사용하는 은닉층의 개수가 다수가 될 경우에는 0에 가까운 기울기가 계속 곱해지면 앞단에서는 거의 기울기를 전파받을 수 없게 된다. 다시 말해 매개변수 W가 업데이트 되지 않아 학습이 되지를 않는다.
- 결론적으로 시그모이드 함수를 은닉층에서 사용하는 것은 지양된다.

## 4. 하이퍼볼릭탄젠트 함수 (Hyperbolic targent function)

![Untitled 2](https://user-images.githubusercontent.com/52296323/125029207-445ba280-e0c4-11eb-82ba-8be2e8d3c7a9.png)

```python
x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = np.tanh(x)

plt.plot(x, y)
plt.plot([0,0],[1.0,-1.0], ':')
plt.axhline(y=0, color='orange', linestyle='--')
plt.title('Tanh Function')
plt.show()
```

- tanh는 입력값을 -1과 1사이의 값으로 변환한다.
- 해당 함수도 -1과 1에 가까운 출력값을 출력할 때, 시그모이드 함수와 같은 문제가 발생한다. 그러나 시그모이드 함수와는 달리 0을 중심으로 하고 있는데, 이 때문에 시그모이드 함수와 비교하면 반환값의 변화폭이 더 크다. 그래서 시그모이드 함수보다는 기울기 소실 증상이 적은 편이다.
- 은닉층에서 시그모이드 함수보다는 많이 사용된다.

## 5. 렐루 함수(ReLU)

![Untitled 3](https://user-images.githubusercontent.com/52296323/125029235-4b82b080-e0c4-11eb-82f7-71f9f7c3fe71.png)

```python
def relu(x):
    return np.maximum(0, x)

x = np.arange(-5.0, 5.0, 0.1)
y = relu(x)

plt.plot(x, y)
plt.plot([0,0],[5.0,0.0], ':')
plt.title('Relu Function')
plt.show()
```

- 인공 신경망에서 가장 최고의 인기를 얻고 있는 함수이다. 수식은 f(x) = max(0,x)로 아주 간단하다.
- 렐루 함수는 음수를 입력하면 0을 출력하고, 양수를 입력하면 입력값을 그대로 반환한다. 렐루 함수는 특정 양수값에 수렴하지 않으므로, 깊은 신경망에서 시그모이드 함수보다 훨씬 더 잘 작동한다.
- 하지만 역시나 문제점이 존재한다. 입력값이 음수면 기울기도 0이 된다. 그리고 이 뉴런은 다시 회생하는 것이 매우 어렵다. 이를 죽은 렐루(dying ReLU) 라고 한다.

## 6. 리키 렐루(Leakt ReLU)

![Untitled 4](https://user-images.githubusercontent.com/52296323/125029257-53425500-e0c4-11eb-93c7-6081f067976d.png)

```python
a = 0.1

def leaky_relu(x):
    return np.maximum(a*x, x)

x = np.arange(-5.0, 5.0, 0.1)
y = leaky_relu(x)

plt.plot(x, y)
plt.plot([0,0],[5.0,0.0], ':')
plt.title('Leaky ReLU Function')
plt.show()
```

- 죽은 렐루를 보완하기 위해 ReLU의 변형 함수들이 등장하기 시작했다. 그 중 Leaky ReLU는 입력값이 음수일 경우에 0이 아니라, 0.001과 같은 매우 작은 수를 반환하도록 되어 있다.
- 여기서 쓰이는 a는 하이퍼파라미터로 Leaky("새는") 정도를 결정하며, 일반적으로는 0.01의 값을 가진다. 여기서 말하는 새는 정도 라는 것은 입력값의 음수 일 때의 기울기를 비유하고 있다.

## 7. 소프트맥스 함수(Softmax function)

![Untitled 5](https://user-images.githubusercontent.com/52296323/125029283-5b01f980-e0c4-11eb-9ba8-f2b5a40a2ed6.png)

```python
x = np.arange(-5.0, 5.0, 0.1) # -5.0부터 5.0까지 0.1 간격 생성
y = np.exp(x) / np.sum(np.exp(x))

plt.plot(x, y)
plt.title('Softmax Function')
plt.show()
```

- 소프트맥스 함수는 시그모이드 함수처럼 출력층의 뉴런에서 주로 사용되는데, 시그모이드 함수가 두 가지 선택지 중하나를 고르는 이진 분류 문제에 사용된다면 3가지 이상의 선택지 중 하나를 고르는 **다중 클래스 분류 문제에 주로 사용**된다.

# 4. 행렬의 곱셈을 이용한 순전파(Forward Propagation)

```python
from keras.models import Sequential
from keras.layers import Dense
model = Sequential() # 층을 추가할 준비
model.add(Dense(8, input_dim=4, activation='relu'))
# 입력층(4)과 다음 은닉층(8) 그리고 은닉층의 활성화 함수는 relu
model.add(Dense(8, activation='relu')) # 은닉층(8)의 활성화 함수는 relu
model.add(Dense(3, activation='softmax'))
```

- 인공 신경망에서 입력층에서 출력층 방향으로 연산을 진행하는 과정을 순전파(Forward Propagation)라고 한다. 다르게 말하면 주어진 입력으로부터 예측값을 계산하는 과정을 순전파라고 한다.
- 머신 러닝에서의 벡터와 행렬 연산을 인공 신경망에 적용하려고 하면, 벡터와 행렬 연산이 순전파 과정에서 층(Layer)마다 적용된다.
- 위와 같이 설정하면 모델의 층은 다음과 같이 나누어진다.

  입력층 : 4개의 입력과 8개의 출력

  은닉층1 : 8개의 입력과 8개의 출력

  은닉층2 : 8개의 입력과 3개의 출력

  출력층 : 3개의 입력과 3개의 출력

## 1. layer 1의 행렬 크기 추정

- layer 1에서 처음 입력으로 들어오는 입력 행렬 X의 크기는 1 X 4 이다.
- layer 1의 출력은 8개 이므로 그에 따라 출력 행렬 Y의 크기는 1 X 8 이다.
- 가중치 행렬 W의 행은 입력 행렬 X의 열과 같아야 하므로 4 X j 크기를 가진다.
- 편향 행렬 B는 출력 행렬 Y의 크기에 영향을 주지 않으므로, 편향 행렬 B의 크기는 출력 행렬 Y의 크기와 같다.
- 가중치 행렬 W의 열은 출력 행렬 Y의 열과 동일해야 한다.

$$X_1\ _*\ _4 \times W_4\ _*\ _8  + B_1\ _* \ _8 = Y_1\ _*\ _8$$

## 2. layer 2와 layer 3의 행렬 크기 추정

- 위에서 계산식을 기준으로 layer 2와 layer 3에서의 가중치 행렬과 편향 행렬 크기를 구할 수 있다. 비록 은닉층과 출력층에 활성화 함수가 존재하지만 활성화 함수는 행렬의 크기에 영향을 주지 않는다.

$$X_1\ _*\ _8 \times W_8\ _*\ _8  + B_1\ _* \ _8 = Y_1\ _*\ _8$$

$$X_1\ _*\ _8 \times W_8\ _*\ _3  + B_1\ _* \ _3 = Y_1\ _*\ _3$$

# 5. 정리

- 이와 같이 인공 신경망이 입력층에서 은닉층을 지나, 출력층에서 예측값을 계산하기 까지의 과정을 행렬 연산으로 가정하고 행렬의 크기를 추정해보았다. 이렇게 순전파를 진행하고 예측값을 구하고 나서 이 다음에 인공 신경망이 해야할 일은 예측값과 실제값으로부터 오차를 계산하고, 오차로부터 가중치와 편향을 업데이트 하는 일이다. (학습 단계)
- 이때 인공신경망은 순전파와는 반대 방향으로 연산을 진행하며 가중치를 업데이트하는데, 이 과정을 역전파(BackPropagation)라고 한다.
